import os
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
import google.generativeai as genai
from dotenv import load_dotenv

# --- Load bi·∫øn m√¥i tr∆∞·ªùng ---
load_dotenv()

app = Flask(__name__)

# --- C·∫•u h√¨nh CORS (ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh tr√™n Render) ---
CORS(
    app,
    resources={r"/*": {"origins": ["https://e-book-for-me.web.app"]}},
    supports_credentials=True,
    allow_headers=["Content-Type", "Authorization"],
    methods=["GET", "POST", "OPTIONS"]
)

# --- Sau m·ªói ph·∫£n h·ªìi, t·ª± th√™m header CORS (ƒë·∫£m b·∫£o kh√¥ng b·ªã ch·∫∑n OPTIONS) ---
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', 'https://e-book-for-me.web.app')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')
    return response

# --- C·∫•u h√¨nh Gemini API ---
try:
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key:
        raise ValueError("‚ö†Ô∏è GEMINI_API_KEY not found in environment variables.")
    
    genai.configure(api_key=gemini_api_key)
    print("‚úÖ Gemini API key loaded successfully.")

    # Li·ªát k√™ c√°c model c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ ki·ªÉm tra tr√™n Render log
    print("üìã Available models:")
    for m in genai.list_models():
        if "generateContent" in m.supported_generation_methods:
            print(" -", m.name)

    # D√πng model m·ªõi, t∆∞∆°ng th√≠ch b·∫£n SDK hi·ªán t·∫°i
    model = genai.GenerativeModel('gemini-flash-latest')

except Exception as e:
    print(f"‚ùå Error configuring Gemini API: {e}")
    model = None


# --- H√†m sinh ph·∫£n h·ªìi stream ---
def generate_response_stream(prompt):
    if not model:
        yield "data: [ERROR] Gemini model is not configured.\n\n"
        return

    try:
        chat_session = model.start_chat(history=[])
        response_stream = chat_session.send_message(prompt, stream=True)

        for chunk in response_stream:
            if chunk.text:
                yield f"data: {chunk.text}\n\n"

    except Exception as e:
        print(f"‚ö†Ô∏è Error during generation: {e}")
        yield f"data: [ERROR] Sorry, an error occurred: {str(e)}\n\n"


# --- API ch√≠nh ---
@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.json or {}
        user_message = data.get('message', '')
        conversation_history = data.get('conversation_history', [])
        document_content = data.get('document_content', '')
        dictionary_content = data.get('dictionary_content', '')
        language = data.get('language', 'vi')

        prompt = f"""
You are an AI assistant for an e-learning platform.
Respond in {'Vietnamese' if language == 'vi' else 'English'}.

Conversation history:
{conversation_history}

Document content:
--- DOCUMENT START ---
{document_content}
--- DOCUMENT END ---

Custom dictionary/glossary:
--- DICTIONARY START ---
{dictionary_content}
--- DICTIONARY END ---

User's latest message: "{user_message}"
"""

        return Response(generate_response_stream(prompt), mimetype='text/event-stream')

    except Exception as e:
        print(f"‚ùå Error in /chat endpoint: {e}")
        return jsonify({"error": "Internal server error occurred."}), 500


# --- Ch·∫°y local ---
if __name__ == '__main__':
    port = int(os.environ.get("PORT", 5000))
    app.run(host='0.0.0.0', port=port, debug=True)
